# Coat Tail Capital ðŸ‹

> "Riding smart money so you don't have to think"

## Project Overview

Real-time whale tracking and alpha scoring platform built with PySpark Structured Streaming on AWS. This is a portfolio project demonstrating Principal-level Big Data Architecture skills.

**Contributors:**
- Mike Veksler â€” Principal Architect, PySpark Lead (GitHub: mveksler)
- Frank D'Avanzo â€” Head of Agentic AI & Strategic Fly-Bys, BMAD-Method Coach (GitHub: TheFrankBuilder)

## Tech Stack

- **Streaming:** Kinesis Data Streams â†’ PySpark Structured Streaming â†’ Apache Iceberg
- **Compute:** EMR Serverless (Spark 3.4)
- **Storage:** S3 Iceberg lakehouse, DynamoDB (alerts), Glue Data Catalog
- **Orchestration:** Step Functions + EventBridge
- **Governance:** Lake Formation
- **IaC:** Terraform (modular, remote state in S3)
- **CI/CD:** GitHub Actions

## Repository Structure

```
coattail-capital/
â”œâ”€â”€ _bmad/                     # BMAD v6 method (agents, workflows, configs)
â”‚   â”œâ”€â”€ _config/               # Manifests, agent customizations
â”‚   â”œâ”€â”€ core/                  # Core platform (bmad-master, brainstorming, party-mode)
â”‚   â””â”€â”€ bmm/                   # BMM module (10 agents, 25 workflows)
â”œâ”€â”€ _bmad-output/              # BMAD artifacts (sprint-status, stories, specs)
â”‚   â”œâ”€â”€ planning-artifacts/    # PRDs, architecture docs, epics
â”‚   â””â”€â”€ implementation-artifacts/ # Story files, tech specs
â”œâ”€â”€ .claude/
â”‚   â””â”€â”€ commands/              # BMAD v6 slash commands (41 commands)
â”œâ”€â”€ agents/                    # Legacy BMAD agent prompts (pre-v6 reference)
â”œâ”€â”€ config/                    # Feature configuration
â”‚   â”œâ”€â”€ features.yaml          # Active module config
â”‚   â””â”€â”€ tiers/                 # Tier definitions (small/medium/large)
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ PRD.md                 # Product requirements (START HERE)
â”‚   â”œâ”€â”€ MODULE_REGISTRY.md     # Feature module catalog (11 modules)
â”‚   â”œâ”€â”€ ARCHITECTURE.md        # System design
â”‚   â”œâ”€â”€ WELL-ARCHITECTED.md    # AWS WAF analysis
â”‚   â”œâ”€â”€ ADR.md                 # Architecture decision records
â”‚   â””â”€â”€ RUNBOOK.md             # Weekend execution guide
â”œâ”€â”€ infra/                     # Terraform infrastructure
â”‚   â”œâ”€â”€ main.tf                # Root module + tier locals + feature SSM
â”‚   â”œâ”€â”€ variables.tf           # Input variables + feature_tier + module toggles
â”‚   â”œâ”€â”€ outputs.tf             # Output values
â”‚   â””â”€â”€ modules/               # Terraform modules (8 modules)
â”œâ”€â”€ src/                       # Application code (to be built)
â”‚   â”œâ”€â”€ producer/              # Kinesis producer
â”‚   â”œâ”€â”€ connectors/            # Data source implementations
â”‚   â”œâ”€â”€ detectors/             # Feature module implementations
â”‚   â”œâ”€â”€ spark-jobs/
â”‚   â”‚   â”œâ”€â”€ framework/         # Plugin framework (base classes)
â”‚   â”‚   â”œâ”€â”€ batch/             # Historical/reprocessing
â”‚   â”‚   â””â”€â”€ common/            # Shared modules
â”‚   â”œâ”€â”€ api/                   # Lambda handlers
â”‚   â””â”€â”€ dashboard/             # React frontend
â”œâ”€â”€ scripts/                   # Operational scripts
â””â”€â”€ tests/                     # Test files (to be built)
```

## Development Workflow

### BMAD v6 Method

This project uses [BMAD v6.0.0-Beta.8](https://github.com/bmad-code-org/BMAD-METHOD) with Claude Code integration. The method provides 10 agents, 25 workflows, and 41 native slash commands.

**Get oriented:**
```
/bmad-help                          # What to do next, which workflow to run
/bmad-party-mode                    # Multi-agent group discussion
```

**Phase 1 â€” Analysis:**
```
/bmad-bmm-create-product-brief      # Business Analyst (Mary) creates product brief
/bmad-bmm-domain-research           # Domain research with web sources
/bmad-bmm-market-research           # Market/competitive research
```

**Phase 2 â€” Planning:**
```
/bmad-bmm-create-prd                # Product Manager (John) creates PRD
/bmad-bmm-validate-prd              # Validate existing PRD against BMAD standards
/bmad-bmm-create-ux-design          # UX Designer (Sally) creates UX spec
```

**Phase 3 â€” Solutioning:**
```
/bmad-bmm-create-architecture       # Architect (Winston) designs system
/bmad-bmm-create-epics-and-stories  # Break PRD into epics and stories
/bmad-bmm-check-implementation-readiness  # Gate check before coding
```

**Phase 4 â€” Implementation:**
```
/bmad-bmm-sprint-planning           # Generate sprint-status.yaml
/bmad-bmm-create-story              # Scrum Master (Bob) prepares next story
/bmad-bmm-dev-story                 # Developer (Amelia) implements story
/bmad-bmm-code-review               # Adversarial code review
/bmad-bmm-retrospective             # Post-epic retrospective
```

**Quick Flow (small tasks):**
```
/bmad-bmm-quick-spec                # Barry creates lean tech spec
/bmad-bmm-quick-dev                 # Barry implements from spec or instructions
```

**Load a specific agent directly:**
```
/bmad-agent-bmm-dev                 # Amelia â€” Senior Software Engineer
/bmad-agent-bmm-architect           # Winston â€” System Architect
/bmad-agent-bmm-sm                  # Bob â€” Scrum Master
/bmad-agent-bmm-qa                  # Quinn â€” QA Engineer
/bmad-agent-bmad-master             # BMad Master â€” Orchestrator
```

> **Note:** Legacy agent prompts are preserved in `agents/` for project-specific reference (especially `data-engineer-agent.md` with PySpark framework specs). BMAD v6 agents live in `_bmad/bmm/agents/`.

### Key Commands

```bash
# Deploy infrastructure
cd infra && terraform init && terraform apply

# Start streaming pipeline
./scripts/start.sh

# Stop streaming (save costs)
./scripts/stop.sh

# Run tests
pytest tests/ -v
```

## Current State

### âœ… Completed (Specs & Infrastructure)
- PRD with modular feature architecture (tiers: Small/Medium/Large)
- Module Registry with 11 feature modules (MOD-001 through MOD-011)
- Architecture document with generic connector â†’ detector â†’ sink pipeline
- Well-Architected Framework review (6 pillars)
- 9 Terraform modules + feature tier system (tier-aware EMR sizing, SSM parameters)
- 7 Architecture Decision Records (ADR-007: Modular Feature Architecture)
- Feature configuration system (config/features.yaml + tier YAMLs)
- Weekend runbook
- GitHub Actions CI
- BMAD v6.0.0-Beta.8 installed with Claude Code integration (10 agents, 25 workflows, 41 slash commands)

### ðŸš§ To Build (Application Code)
- [ ] Module framework (BaseConnector, BaseDetector, AlertRouter, ModuleRegistry, ConfigLoader, PipelineRunner)
- [ ] Connector implementations (Binance, Coinbase as BaseConnector subclasses)
- [ ] Small tier detectors (volume-anomaly, whale-detector, spread-calculator)
- [ ] Config-driven data quality module
- [ ] Kinesis producer with connector manager
- [ ] Batch jobs (historical loader, reprocessor)
- [ ] Lambda API handlers
- [ ] React dashboard

### ðŸ”® Future Phases
- [ ] On-chain whale tracking (Ethereum, Solana)
- [ ] Wallet alpha scoring
- [ ] Signal generation
- [ ] Hyperliquid execution engine

## Coding Standards

### Python
- Python 3.11+
- Type hints required
- Docstrings for public functions
- Use `ruff` for linting
- Use `mypy` for type checking
- Use `pytest` for testing

### PySpark
- DataFrame API only (no RDDs)
- Structured Streaming with checkpointing
- Watermarking for late data
- Iceberg sinks via Glue Catalog

### Terraform
- Modular design (one module per service group)
- All resources tagged
- Remote state in S3 with DynamoDB locking
- No hardcoded values

## Important Files to Read First

1. `docs/PRD.md` â€” Full requirements, schemas, data quality specs
2. `docs/ADR.md` â€” Why we chose Kinesis over MSK, Iceberg over Delta, etc.
3. `docs/RUNBOOK.md` â€” Hour-by-hour weekend execution plan
4. `agents/data-engineer-agent.md` â€” Detailed specs for PySpark jobs (legacy, still authoritative for framework)
5. `_bmad/_config/workflow-manifest.csv` â€” All available BMAD workflows
6. `_bmad/_config/agent-manifest.csv` â€” All available BMAD agents

## Environment Variables

```bash
# AWS
export AWS_REGION=us-west-2
export AWS_PROFILE=default  # or your profile

# Kinesis Producer
export KINESIS_STREAM_NAME=coattail-trades
export SYMBOLS=btcusdt,ethusdt,solusdt

# Spark Jobs (set via Terraform outputs)
export CHECKPOINT_BUCKET=coattail-dev-checkpoint-{account_id}
export PROCESSED_BUCKET=coattail-dev-processed-{account_id}
export ALERTS_TABLE=coattail-dev-alerts
export GLUE_DATABASE=coattail_dev_lakehouse
```

## Testing

```bash
# Unit tests
pytest tests/unit/ -v

# Integration tests (requires deployed infrastructure)
pytest tests/integration/ -v --tb=short

# Specific test file
pytest tests/unit/test_volume_anomaly.py -v
```

## Deployment

```bash
# First time setup
./scripts/bootstrap-state.sh
cd infra
terraform init

# Deploy all infrastructure
terraform plan -out=plan.tfplan
terraform apply plan.tfplan

# View outputs (stream names, bucket names, etc.)
terraform output

# Destroy when done
terraform destroy
```

## Cost Control

- Billing alarm at $25 (auto-configured)
- Use `./scripts/stop.sh` when not demoing
- EMR Serverless auto-stops after 15 min idle
- DynamoDB TTL expires alerts after 24h
- S3 lifecycle moves to IA after 30d, deletes after 90d

## Links

- **PRD:** `docs/PRD.md`
- **Architecture:** `docs/ARCHITECTURE.md`
- **Runbook:** `docs/RUNBOOK.md`
- **Mike's LinkedIn:** https://www.linkedin.com/in/mikeveksler-798b7913
- **Frank's GitHub:** https://github.com/TheFrankBuilder
