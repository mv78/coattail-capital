# Coat Tail Capital ðŸ‹

> "Riding smart money so you don't have to think"

## Project Overview

Real-time whale tracking and alpha scoring platform built with PySpark Structured Streaming on AWS. This is a portfolio project demonstrating Principal-level Big Data Architecture skills.

**Contributors:**
- Mike Veksler â€” Principal Architect, PySpark Lead (GitHub: mveksler)
- Frank D'Avanzo â€” Engineering Manager, BMAD Coach (GitHub: TheFrankBuilder)

## Tech Stack

- **Streaming:** Kinesis Data Streams â†’ PySpark Structured Streaming â†’ Apache Iceberg
- **Compute:** EMR Serverless (Spark 3.4)
- **Storage:** S3 Iceberg lakehouse, DynamoDB (alerts), Glue Data Catalog
- **Orchestration:** Step Functions + EventBridge
- **Governance:** Lake Formation
- **IaC:** Terraform (modular, remote state in S3)
- **CI/CD:** GitHub Actions

## Repository Structure

```
coattail-capital/
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ PRD.md                 # Product requirements (START HERE)
â”‚   â”œâ”€â”€ ARCHITECTURE.md        # System design (generate this)
â”‚   â”œâ”€â”€ WELL-ARCHITECTED.md    # AWS WAF analysis (generate this)
â”‚   â”œâ”€â”€ ADR.md                 # Architecture decision records
â”‚   â””â”€â”€ RUNBOOK.md             # Weekend execution guide
â”œâ”€â”€ agents/                    # BMAD agent prompts
â”‚   â”œâ”€â”€ ba-agent.md            # Business Analyst
â”‚   â”œâ”€â”€ architect-agent.md     # Solutions Architect
â”‚   â”œâ”€â”€ data-engineer-agent.md # Data Engineer
â”‚   â”œâ”€â”€ security-agent.md      # Security Engineer
â”‚   â”œâ”€â”€ devops-agent.md        # DevOps Engineer
â”‚   â””â”€â”€ qa-agent.md            # QA Engineer
â”œâ”€â”€ infra/                     # Terraform infrastructure
â”‚   â”œâ”€â”€ main.tf                # Root module composition
â”‚   â”œâ”€â”€ variables.tf           # Input variables
â”‚   â”œâ”€â”€ outputs.tf             # Output values
â”‚   â””â”€â”€ modules/               # Terraform modules
â”‚       â”œâ”€â”€ kinesis/
â”‚       â”œâ”€â”€ s3-lakehouse/
â”‚       â”œâ”€â”€ emr-serverless/
â”‚       â”œâ”€â”€ glue/
â”‚       â”œâ”€â”€ iam/
â”‚       â”œâ”€â”€ monitoring/
â”‚       â”œâ”€â”€ step-functions/
â”‚       â””â”€â”€ lake-formation/
â”œâ”€â”€ src/                       # Application code (to be built)
â”‚   â”œâ”€â”€ producer/              # Kinesis producer
â”‚   â”œâ”€â”€ spark-jobs/
â”‚   â”‚   â”œâ”€â”€ streaming/         # Real-time jobs
â”‚   â”‚   â”œâ”€â”€ batch/             # Historical/reprocessing
â”‚   â”‚   â””â”€â”€ common/            # Shared modules
â”‚   â”œâ”€â”€ api/                   # Lambda handlers
â”‚   â””â”€â”€ dashboard/             # React frontend
â”œâ”€â”€ scripts/                   # Operational scripts
â”‚   â”œâ”€â”€ start.sh
â”‚   â”œâ”€â”€ stop.sh
â”‚   â””â”€â”€ bootstrap-state.sh
â””â”€â”€ tests/                     # Test files (to be built)
```

## Development Workflow

### Using BMAD Agents

This project uses BMAD (Business-Manager-Architect-Developer) method. Each agent in `agents/` is a specialized prompt. To use an agent:

```bash
# Load an agent's context and give it a task
claude "$(cat agents/data-engineer-agent.md)

Build the Kinesis producer in src/producer/ that connects to Binance WebSocket..."
```

### Key Commands

```bash
# Deploy infrastructure
cd infra && terraform init && terraform apply

# Start streaming pipeline
./scripts/start.sh

# Stop streaming (save costs)
./scripts/stop.sh

# Run tests
pytest tests/ -v
```

## Current State

### âœ… Completed (Specs & Infrastructure)
- PRD with data quality, batch processing sections
- 6 BMAD agent specifications
- 9 Terraform modules (Kinesis, S3, EMR, Glue, IAM, Monitoring, Step Functions, Lake Formation)
- 6 Architecture Decision Records
- Weekend runbook
- GitHub Actions CI

### ðŸš§ To Build (Application Code)
- [ ] Kinesis producer (Binance + Coinbase WebSocket)
- [ ] PySpark streaming jobs (volume anomaly, whale detector, spread calculator)
- [ ] Data quality module
- [ ] Batch jobs (historical loader, reprocessor)
- [ ] Lambda API handlers
- [ ] React dashboard

### ðŸ”® Future Phases
- [ ] On-chain whale tracking (Ethereum, Solana)
- [ ] Wallet alpha scoring
- [ ] Signal generation
- [ ] Hyperliquid execution engine

## Coding Standards

### Python
- Python 3.11+
- Type hints required
- Docstrings for public functions
- Use `ruff` for linting
- Use `mypy` for type checking
- Use `pytest` for testing

### PySpark
- DataFrame API only (no RDDs)
- Structured Streaming with checkpointing
- Watermarking for late data
- Iceberg sinks via Glue Catalog

### Terraform
- Modular design (one module per service group)
- All resources tagged
- Remote state in S3 with DynamoDB locking
- No hardcoded values

## Important Files to Read First

1. `docs/PRD.md` â€” Full requirements, schemas, data quality specs
2. `docs/ADR.md` â€” Why we chose Kinesis over MSK, Iceberg over Delta, etc.
3. `docs/RUNBOOK.md` â€” Hour-by-hour weekend execution plan
4. `agents/data-engineer-agent.md` â€” Detailed specs for PySpark jobs

## Environment Variables

```bash
# AWS
export AWS_REGION=us-west-2
export AWS_PROFILE=default  # or your profile

# Kinesis Producer
export KINESIS_STREAM_NAME=coattail-trades
export SYMBOLS=btcusdt,ethusdt,solusdt

# Spark Jobs (set via Terraform outputs)
export CHECKPOINT_BUCKET=coattail-dev-checkpoint-{account_id}
export PROCESSED_BUCKET=coattail-dev-processed-{account_id}
export ALERTS_TABLE=coattail-dev-alerts
export GLUE_DATABASE=coattail_dev_lakehouse
```

## Testing

```bash
# Unit tests
pytest tests/unit/ -v

# Integration tests (requires deployed infrastructure)
pytest tests/integration/ -v --tb=short

# Specific test file
pytest tests/unit/test_volume_anomaly.py -v
```

## Deployment

```bash
# First time setup
./scripts/bootstrap-state.sh
cd infra
terraform init

# Deploy all infrastructure
terraform plan -out=plan.tfplan
terraform apply plan.tfplan

# View outputs (stream names, bucket names, etc.)
terraform output

# Destroy when done
terraform destroy
```

## Cost Control

- Billing alarm at $25 (auto-configured)
- Use `./scripts/stop.sh` when not demoing
- EMR Serverless auto-stops after 15 min idle
- DynamoDB TTL expires alerts after 24h
- S3 lifecycle moves to IA after 30d, deletes after 90d

## Links

- **PRD:** `docs/PRD.md`
- **Architecture:** `docs/ARCHITECTURE.md` (to be generated)
- **Runbook:** `docs/RUNBOOK.md`
- **Mike's LinkedIn:** https://www.linkedin.com/in/mikeveksler-798b7913
- **Frank's GitHub:** https://github.com/TheFrankBuilder
